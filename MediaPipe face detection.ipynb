{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b9a33d",
   "metadata": {},
   "source": [
    "### MediaPipe is an open-source framework developed by google that offers customizable machine learning (ML) pipelines to process media data such as image, video and audio. It provides a wide range of pre-trained ML models and components to preform various tasks,including:\n",
    "\n",
    "### 1. Pose estimation: Detecting and tracking human body poses in images and video.\n",
    "\n",
    "### 2. Hand Tracking: Identifying and tracking hands in images and video.\n",
    "\n",
    "### 3. Object Detection: Detecting and tracking objects in images and video streams.\n",
    "\n",
    "### 4. Face Detection: Detecting and tracking faces in images and video.\n",
    "\n",
    "### 5. Face Mesh: Estimating facial landmarks in real-time.\n",
    "\n",
    "### 6. Holistic: Combining multiple components like face detection, pose estimation, and hand tracking to create holistic pipelines.\n",
    "\n",
    "### 7. Selfie Segmentation: Segmenting a person's image from the background in real-time.\n",
    "\n",
    "### 8. Hair Segmentation: Segmenting hair from images and video.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c7200a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.11.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe  --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681bb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0116e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_facedetection=mp.solutions.face_detection\n",
    "mp_drawings=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feecc99a",
   "metadata": {},
   "source": [
    "### mp_facedetection=mp.solutions.face_detection: This line  imports the face detection module from the Mediapipe library and assigns it to the variable mp_facedetection. This module provides functionality for detecting faces in images or video streams \n",
    "\n",
    "### mp_drawings=mp.solutions.drawing_utils: This line imports the drawing utilities module from the Mediapipe library and assigns it to the variable mp_drawings. This module contains functions for drawing annotations, such as bounding boxes or landmarks, on images or video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a2fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection=mp_facedetection.FaceDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8eb852",
   "metadata": {},
   "source": [
    "### The Face Detection class in the Mediapipe library utilizes a pre-trained machine learning model to detect faces in images or video frames. This model has been trained on a large dataset of annotated images, allowing it to leam features and patterns associated with human faces. creating an instance of the face detection model from the Mediapipe library. The line face_detection=mp_facedetection.FaceDetection) initializes an instance of the Face Detection class, This instance of the FaceDetection class will allow to perform face detection on images or video streams using the functionality provided by the Mediapipe Ilbrary, then use this face_detection object to detect faces in images or video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43106646",
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    suc,img=video.read() # if the face is successfully detected from the video it will store in suc.\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # converted to bgr to rgb .\n",
    "    result=face_detection.process(img)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    if result.detections:\n",
    "        for det in result.detections:\n",
    "            \n",
    "            mp_drawings.draw_detection(img,det)\n",
    "    cv2.imshow('FACE',img)\n",
    "    if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc17c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[label_id: 0\n",
       " score: 0.954870522\n",
       " location_data {\n",
       "   format: RELATIVE_BOUNDING_BOX\n",
       "   relative_bounding_box {\n",
       "     xmin: 0.339052945\n",
       "     ymin: 0.629968\n",
       "     width: 0.23928991\n",
       "     height: 0.319052458\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.413364828\n",
       "     y: 0.716967285\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.51434356\n",
       "     y: 0.709827423\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.472680628\n",
       "     y: 0.783248425\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.472461581\n",
       "     y: 0.852254093\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.348295361\n",
       "     y: 0.758256733\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.564307928\n",
       "     y: 0.742939472\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.detections #label_id is 0 it means face is delected succesfully # 1st & 2Nd relative keypointis two eyes then nose ,lip, 2 ears are the order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2990d26",
   "metadata": {},
   "source": [
    "##### face detection process this code would typically detect faces in the image and store the result in the variable result.\n",
    "\n",
    "##### Reading a Frame: suc, img = video.read() captures a frame from the video feed.\n",
    "\n",
    "##### Color Conversion (BGR to RGB): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) converts the image from BGR (OpenCV's delault) to RGB (required by MediaPipe).\n",
    "\n",
    "##### Face Detection: result = face_detection.process(img) processes the image to detect faces.\n",
    "\n",
    "##### Color Conversion (RGB to BGR): img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) converts the image back to BGR for display with OpenCV.\n",
    "\n",
    "##### Drawing Face Detections: If faces are detected (if result.detections), a loop iterates through each detected face.\n",
    "\n",
    "##### mp_drawings.draw_detection(img, det) draws the detected face on the image.\n",
    "\n",
    "##### Displaying the Image: cv2.imshow('FACE', img) displays the image with the detected faces.\n",
    "\n",
    "##### Exit Condition: if cv2.waitKey(1) & 0xFF == ord('q): breaks the loop and exits if the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7101c40",
   "metadata": {},
   "source": [
    "### cv2.waitKey(1): Waits for 1 millisecond for a key event.\n",
    "\n",
    "### cv2.waitKey(1) & 0xFF: The bitwise AND operation ensures the result is within the ASCII range.\n",
    "\n",
    "### == ord('q'): Compares the result to the ASCII value of 'q' to check if 'q' was pressed.\n",
    "\n",
    "### If 'q' is pressed, the loop breaks, ending the video capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ef1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
